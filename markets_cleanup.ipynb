{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market details saved to brooklyn_markets.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Scrape market details from website\n",
    "def scrape_market_details(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        market_details = []\n",
    "        # Find all market information data\n",
    "        market_infos = soup.find_all('div', class_='sqs-html-content')\n",
    "        for info in market_infos:\n",
    "            market_info = {}\n",
    "            # Extract the market name from the <h3> tag \n",
    "            h3_tag = info.find('h3')\n",
    "            if h3_tag:\n",
    "                market_info['Name'] = h3_tag.get_text().strip()\n",
    "                # Extract other details from the <p> tags\n",
    "                paragraphs = info.find_all('p')\n",
    "                for paragraph in paragraphs:\n",
    "                    text = paragraph.get_text().strip()\n",
    "                    # Extract location, day, and hours\n",
    "                    if \"Location:\" in text:\n",
    "                        location_text = text.replace(\"Location:\", \"\").strip()\n",
    "                        # Use regular expression to find day and hours pattern in location text\n",
    "                        day_hours_pattern = re.search(r'Hours: ((?:Mon|Tues|Wed|Thurs|Fri|Sat|Sun)s?(?: & (?:Mon|Tues|Wed|Thurs|Fri|Sat|Sun)s?)?) (\\d{1,2}[ap]m - \\d{1,2}[ap]m)', location_text)\n",
    "                        if day_hours_pattern:\n",
    "                            market_info[\"Day\"] = day_hours_pattern.group(1)\n",
    "                            market_info[\"Hours\"] = day_hours_pattern.group(2)\n",
    "                            # Remove the day and hours from the location text\n",
    "                            location_text = location_text.replace(f\"Hours: {market_info['Day']} {market_info['Hours']}\", \"\").strip()\n",
    "                        else:\n",
    "                            market_info[\"Day\"] = None\n",
    "                            market_info[\"Hours\"] = None\n",
    "                        market_info[\"Location\"] = location_text\n",
    "                        # Check for cooking demonstrations\n",
    "                        if \"*Cooking Demonstrations\" in text:\n",
    "                            market_info[\"Cooking Demonstrations\"] = \"Yes\"\n",
    "                        else:\n",
    "                            market_info[\"Cooking Demonstrations\"] = \"No\"\n",
    "                        market_details.append(market_info)\n",
    "        return market_details\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from URL: {url}\")\n",
    "        return []\n",
    "\n",
    "# Define the URL for Brooklyn Markets\n",
    "url = \"https://www.harvesthomefm.org/brooklyn-markets\"\n",
    "\n",
    "# Scrape market details\n",
    "brooklyn_markets = scrape_market_details(url)\n",
    "\n",
    "# Write market details to a CSV file\n",
    "with open(\"brooklyn_markets.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    fieldnames = [\"Name\", \"Location\", \"Day\", \"Hours\", \"Cooking Demonstrations\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for market in brooklyn_markets:\n",
    "        writer.writerow(market)\n",
    "\n",
    "print(\"Market details saved to brooklyn_markets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market details saved to bronx_markets.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Scrape market details from website\n",
    "def scrape_market_details(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        market_details = []\n",
    "        # Find all market information data\n",
    "        market_infos = soup.find_all('div', class_='sqs-html-content')\n",
    "        for info in market_infos:\n",
    "            market_info = {}\n",
    "            # Extract the market name from the <h3> tag \n",
    "            h3_tag = info.find('h3')\n",
    "            if h3_tag:\n",
    "                market_info['Name'] = h3_tag.get_text().strip()\n",
    "                # Extract other details from the <p> tags\n",
    "                paragraphs = info.find_all('p')\n",
    "                for paragraph in paragraphs:\n",
    "                    text = paragraph.get_text().strip()\n",
    "                    # Extract location, day, and hours\n",
    "                    if \"Location:\" in text:\n",
    "                        location_text = text.replace(\"Location:\", \"\").strip()\n",
    "                        # Use regular expression to find day and hours pattern in location text\n",
    "                        day_hours_pattern = re.search(r'Hours: ((?:Mon|Tues|Wed|Thurs|Fri|Sat|Sun)s?(?: & (?:Mon|Tues|Wed|Thurs|Fri|Sat|Sun)s?)?) (\\d{1,2}[ap]m - \\d{1,2}[ap]m)', location_text)\n",
    "                        if day_hours_pattern:\n",
    "                            market_info[\"Day\"] = day_hours_pattern.group(1)\n",
    "                            market_info[\"Hours\"] = day_hours_pattern.group(2)\n",
    "                            # Remove the day and hours from the location text\n",
    "                            location_text = location_text.replace(f\"Hours: {market_info['Day']} {market_info['Hours']}\", \"\").strip()\n",
    "                        else:\n",
    "                            market_info[\"Day\"] = None\n",
    "                            market_info[\"Hours\"] = None\n",
    "                        market_info[\"Location\"] = location_text\n",
    "                        # Check for cooking demonstrations\n",
    "                        if \"*Cooking Demonstrations\" in text:\n",
    "                            market_info[\"Cooking Demonstrations\"] = \"Yes\"\n",
    "                        else:\n",
    "                            market_info[\"Cooking Demonstrations\"] = \"No\"\n",
    "                        market_details.append(market_info)\n",
    "        return market_details\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from URL: {url}\")\n",
    "        return []\n",
    "\n",
    "# Define the URL for Brooklyn Markets\n",
    "url = \"https://www.harvesthomefm.org/bronx-markets\"\n",
    "\n",
    "# Scrape market details\n",
    "bronx_markets = scrape_market_details(url)\n",
    "\n",
    "# Write market details to a CSV file\n",
    "with open(\"bronx_markets.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    fieldnames = [\"Name\", \"Location\", \"Day\", \"Hours\", \"Cooking Demonstrations\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for market in bronx_markets:\n",
    "        writer.writerow(market)\n",
    "\n",
    "print(\"Market details saved to bronx_markets.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market details saved to manhattan_markets.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Scrape market details from website\n",
    "def scrape_market_details(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        market_details = []\n",
    "        # Find all market information data\n",
    "        market_infos = soup.find_all('div', class_='sqs-html-content')\n",
    "        for info in market_infos:\n",
    "            market_info = {}\n",
    "            # Extract the market name from the <h3> tag \n",
    "            h3_tag = info.find('h3')\n",
    "            if h3_tag:\n",
    "                market_info['Name'] = h3_tag.get_text().strip()\n",
    "                # Extract other details from the <p> tags\n",
    "                paragraphs = info.find_all('p')\n",
    "                for paragraph in paragraphs:\n",
    "                    text = paragraph.get_text().strip()\n",
    "                    # Extract location, day, and hours\n",
    "                    if \"Location:\" in text:\n",
    "                        location_text = text.replace(\"Location:\", \"\").strip()\n",
    "                        # Use regular expression to find day and hours pattern in location text\n",
    "                        day_hours_pattern = re.search(r'Hours: ((?:Mon|Tues|Wed|Thurs|Fri|Sat|Sun)s?(?: & (?:Mon|Tues|Wed|Thurs|Fri|Sat|Sun)s?)?) (\\d{1,2}[ap]m - \\d{1,2}[ap]m)', location_text)\n",
    "                        if day_hours_pattern:\n",
    "                            market_info[\"Day\"] = day_hours_pattern.group(1)\n",
    "                            market_info[\"Hours\"] = day_hours_pattern.group(2)\n",
    "                            # Remove the day and hours from the location text\n",
    "                            location_text = location_text.replace(f\"Hours: {market_info['Day']} {market_info['Hours']}\", \"\").strip()\n",
    "                        else:\n",
    "                            market_info[\"Day\"] = None\n",
    "                            market_info[\"Hours\"] = None\n",
    "                        market_info[\"Location\"] = location_text\n",
    "                        # Check for cooking demonstrations\n",
    "                        if \"*Cooking Demonstrations\" in text:\n",
    "                            market_info[\"Cooking Demonstrations\"] = \"Yes\"\n",
    "                        else:\n",
    "                            market_info[\"Cooking Demonstrations\"] = \"No\"\n",
    "                        market_details.append(market_info)\n",
    "        return market_details\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from URL: {url}\")\n",
    "        return []\n",
    "\n",
    "# Define the URL for Brooklyn Markets\n",
    "url = \"https://www.harvesthomefm.org/manhattan-markets\"\n",
    "\n",
    "# Scrape market details\n",
    "manhattan_markets = scrape_market_details(url)\n",
    "\n",
    "# Write market details to a CSV file\n",
    "with open(\"manhattan_markets.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    fieldnames = [\"Name\", \"Location\", \"Day\", \"Hours\", \"Cooking Demonstrations\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for market in manhattan_markets:\n",
    "        writer.writerow(market)\n",
    "\n",
    "print(\"Market details saved to manhattan_markets.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
